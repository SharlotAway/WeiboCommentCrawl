{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rich.progress import track\n",
    "from tqdm import tqdm\n",
    "from utils.__init__ import *\n",
    "import logging\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files(input_folder, output_file):\n",
    "    \"\"\"\n",
    "    将指定文件夹中的所有 CSV 文件读入并合并保存为一个新的 CSV 文件。\n",
    "    \n",
    "    参数:\n",
    "    input_folder (str): 包含 CSV 文件的文件夹路径\n",
    "    output_file (str): 合并后的输出 CSV 文件路径\n",
    "    \"\"\"\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    \n",
    "    # 检查是否存在 CSV 文件\n",
    "    if not csv_files:\n",
    "        print(f\"没有找到 CSV 文件在文件夹: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    # 逐个读取 CSV 文件并添加到列表中\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    # 如果输出文件已存在，将其加入合并列表中\n",
    "    # if os.path.exists(output_file):\n",
    "    #     exist_df = pd.read_csv(output_file)\n",
    "    #     dataframes.append(exist_df)\n",
    "\n",
    "    # 合并所有 DataFrame\n",
    "    # combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    # combined_df.reset_index(drop=True)\n",
    "    # combined_df = combined_df.drop_duplicates()\n",
    "    combined_df = pd.concat(dataframes).reset_index(drop=True).drop_duplicates()\n",
    "    combined_df = combined_df.apply(lambda col: col.fillna(0).astype(int) if col.dtype=='float64' else col)\n",
    "    if \"Unnamed: 0\" in combined_df.columns:\n",
    "        combined_df = combined_df.drop(columns=[\"Unnamed: 0\"])\n",
    "    print(f\"There are {len(combined_df)} comments in total.\")\n",
    "    # 保存合并后的 DataFrame 到新的 CSV 文件\n",
    "    combined_df.to_csv(output_file, index=False, encoding=\"utf_8_sig\")\n",
    "    # print(f\"所有 CSV 文件已合并并保存为 {output_file}\")\n",
    "    return len(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫信息解析类，用于获取微博、一级评论、二级评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WBParser:\n",
    "    def __init__(self, cookie, folder=\"WBData\"):\n",
    "        self.cookie = cookie\n",
    "        # 创建目录结构\n",
    "        self._create_directories(folder)\n",
    "        self.main_body_filepath = f\"./{folder}/demo.csv\"\n",
    "        self.comments_level_1_filename = f\"./{folder}/demo_comments_one.csv\"\n",
    "        self.comments_level_2_filename = f\"./{folder}/demo_comments_two.csv\"\n",
    "        self.comments_level_1_dirpath = f\"./{folder}/Comments_level_1/\"\n",
    "        self.comments_level_2_dirpath = f\"./{folder}/Comments_level_2/\"\n",
    "        # logging.info(f\"Successfully created root path: {folder}!\")\n",
    "        logging.info(f\"Start parsing topic: {folder}!\")\n",
    "\n",
    "    def _create_directories(self, folder):\n",
    "        \"\"\"Helper function to create necessary directories.\"\"\"\n",
    "        os.makedirs(f\"./{folder}\", exist_ok=True)\n",
    "        os.makedirs(f\"./{folder}/Comments_level_1\", exist_ok=True)\n",
    "        os.makedirs(f\"./{folder}/Comments_level_2\", exist_ok=True)\n",
    "\n",
    "    def get_main_body(self, q, kind):\n",
    "        \"\"\"Fetch main body and save it to CSV.\"\"\"\n",
    "        try:\n",
    "            data = get_all_main_body(q, kind, self.cookie)\n",
    "            data = data.reset_index(drop=True).astype(str).drop_duplicates()\n",
    "            data.to_csv(self.main_body_filepath, encoding=\"utf_8_sig\")\n",
    "            logging.info(f\"Main body data saved to {self.main_body_filepath}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching main body: {e}\")\n",
    "        \n",
    "\n",
    "    def get_comments_level_one(self):\n",
    "        \"\"\"Fetch level 1 comments and save them to CSV and folder.\"\"\"\n",
    "        data_list = []\n",
    "        if not os.path.exists(self.main_body_filepath):\n",
    "            logging.error(\"Main body not found, please fetch it first!\")\n",
    "            return\n",
    "\n",
    "        main_body = pd.read_csv(self.main_body_filepath, index_col=0)\n",
    "        logging.info(f\"Found {main_body.shape[0]} main body entries, starting level 1 comments parsing...\")\n",
    "\n",
    "        try:\n",
    "            with tqdm(total=len(range(main_body.shape[0])), desc=\"Parsing main body\") as pbar:\n",
    "                for ix in range(main_body.shape[0]):\n",
    "                    # pbar.set_description(f\"Processing {main_body.iloc[ix][\"mid\"]}\")\n",
    "                # for ix in tqdm(range(main_body.shape[0]), desc=\"Parsing main body\"):\n",
    "                    uid = int(float(main_body.iloc[ix][\"uid\"]))\n",
    "                    mid = int(float(main_body.iloc[ix][\"mid\"]))\n",
    "                    final_file_path = f\"{self.comments_level_1_dirpath}{uid}_{mid}.csv\"\n",
    "                    # print(main_body.iloc[ix][\"mid\"], 1)\n",
    "\n",
    "                    is_exist = self._check_existing_file(final_file_path)\n",
    "                    pbar.set_description(f\"Processing {main_body.iloc[ix][\"mid\"]}, exists={is_exist}\")\n",
    "                    # 如果文件已经存在且不为空，跳过\n",
    "                    if not is_exist:\n",
    "                        # print(main_body.iloc[ix][\"mid\"], 2)\n",
    "                        data = get_all_level_one(uid=uid, mid=mid, cookie=self.cookie)\n",
    "                        # print(main_body.iloc[ix][\"mid\"], 3)\n",
    "                        data.drop_duplicates(inplace=True)\n",
    "                        data.to_csv(final_file_path, encoding=\"utf_8_sig\")\n",
    "                        # print(main_body.iloc[ix][\"mid\"], 4)\n",
    "                        data_list.append(data)\n",
    "                    pbar.update(1)\n",
    "\n",
    "            logging.info(f\"Completed parsing all {main_body.shape[0]} main body entries for level 1 comments.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during level 1 comments parsing: {e}\")\n",
    "\n",
    "        self._save_combined_data(data_list, self.comments_level_1_filename)\n",
    "\n",
    "    def get_comments_level_two(self):\n",
    "        \"\"\"Fetch level 2 comments and save them to CSV and folder.\"\"\"\n",
    "        data_list = []\n",
    "        comments_level_1_data = self._load_level_1_comments()\n",
    "        if comments_level_1_data is None:\n",
    "            logging.error(\"Level 1 comments not found, please fetch them first!\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Found {comments_level_1_data.shape[0]} level 1 comments, starting level 2 parsing...\")\n",
    "\n",
    "        try:\n",
    "            # for ix in tqdm(range(comments_level_1_data.shape[0]), desc=\"Parsing level 1 comments\"):\n",
    "            with tqdm(total=comments_level_1_data.shape[0], desc=\"解析一级评论\") as pbar:\n",
    "                for ix in range(comments_level_1_data.shape[0]):\n",
    "                    pbar.set_description(f\"processing {comments_level_1_data.iloc[ix]['mid']}\")\n",
    "                    main_body_uid = int(float(comments_level_1_data.iloc[ix][\"main_body_uid\"]))\n",
    "                    mid = int(float(comments_level_1_data.iloc[ix][\"mid\"]))\n",
    "                    final_file_path = f\"{self.comments_level_2_dirpath}{main_body_uid}_{mid}.csv\"\n",
    "                    # 如果文件已经存在且不为空，跳过\n",
    "                    if not self._check_existing_file(final_file_path):\n",
    "                        data = get_all_level_two(uid=main_body_uid, mid=mid, cookie=self.cookie)\n",
    "                        data.drop_duplicates(inplace=True)\n",
    "                        data.to_csv(final_file_path, encoding=\"utf_8_sig\")\n",
    "                        data_list.append(data)\n",
    "                    pbar.update(1)\n",
    "\n",
    "            logging.info(f\"Completed parsing all {comments_level_1_data.shape[0]} level 1 comments for level 2 comments.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during level 2 comments parsing: {e}\")\n",
    "\n",
    "        self._save_combined_data(data_list, self.comments_level_2_filename)\n",
    "\n",
    "    def _load_level_1_comments(self):\n",
    "        \"\"\"Load level 1 comments from the CSV file or directory.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.comments_level_1_filename):\n",
    "                return pd.read_csv(self.comments_level_1_filename, index_col=0)\n",
    "            else:\n",
    "                file_list = [self.comments_level_1_dirpath + item for item in os.listdir(self.comments_level_1_dirpath) if item.endswith('.csv')]\n",
    "                if file_list:\n",
    "                    return pd.concat([pd.read_csv(file) for file in file_list]).reset_index(drop=True).astype(str).drop_duplicates()\n",
    "                else:\n",
    "                    return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading level 1 comments: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _check_existing_file(self, filepath):\n",
    "        \"\"\"Check if a CSV file exists and is not empty.\"\"\"\n",
    "        if os.path.exists(filepath):\n",
    "            length = pd.read_csv(filepath).shape[0]\n",
    "            if length > 0:\n",
    "                # logging.info(f\"Skipping existing file: {filepath}, already has {length} entries.\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _save_combined_data(self, data_list, filename):\n",
    "        \"\"\"Save combined data from the list to a CSV file.\"\"\"\n",
    "        if data_list:\n",
    "            data = pd.concat(data_list).reset_index(drop=True).astype(str).drop_duplicates()\n",
    "            data.to_csv(filename, encoding='utf_8_sig')\n",
    "            logging.info(f\"Data successfully saved to {filename}.\")\n",
    "        else:\n",
    "            logging.warning(f\"No data to save to {filename}.\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对指定主题进行爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = [\n",
    "    \"#胖猫姐姐刘某操控舆论#\",\n",
    "    \"#胖猫事件带给我们的反思才刚刚开始#\",\n",
    "    \"#胖猫案细节公布#\",\n",
    "    \"#胖猫事件#\",\n",
    "    \"#警方通报胖猫事件调查情况#\",\n",
    "    \"#胖猫谭竹#\"\n",
    "]\n",
    "kind = \"综合\"  # 综合，实时，热门，高级\n",
    "cookie = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully created root path: #胖猫姐姐刘某操控舆论#!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:话题：#胖猫姐姐刘某操控舆论#，类型：综合，解析成功，一共有42页，准备开始解析...\n",
      "解析中...: 100%|██████████| 41/41 [00:20<00:00,  2.00it/s]\n",
      "INFO:root:话题：#胖猫姐姐刘某操控舆论#，类型：综合，一共有42页，已经解析完毕！\n",
      "INFO:root:Main body data saved to ./#胖猫姐姐刘某操控舆论#/demo.csv.\n",
      "INFO:root:Found 414 main body entries, starting level 1 comments parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing main body: 100%|██████████| 414/414 [41:22<00:00,  6.00s/it]   \n",
      "INFO:root:Completed parsing all 414 main body entries for level 1 comments.\n",
      "INFO:root:Data successfully saved to ./#胖猫姐姐刘某操控舆论#/demo_comments_one.csv.\n",
      "INFO:root:Found 6663 level 1 comments, starting level 2 parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing level 1 comments:   0%|          | 2/6663 [16:34<920:23:29, 497.43s/it] \n",
      "ERROR:root:Error during level 2 comments parsing: 解析页面失败，请检查你的cookie是否正确！\n",
      "INFO:root:Data successfully saved to ./#胖猫姐姐刘某操控舆论#/demo_comments_two.csv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minutes break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:00<00:00,  1.00s/it]\n",
      "INFO:root:Successfully created root path: #胖猫事件带给我们的反思才刚刚开始#!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:话题：#胖猫事件带给我们的反思才刚刚开始#，类型：综合，解析成功，一共有 8页，准备开始解析...\n",
      "解析中...: 100%|██████████| 7/7 [00:03<00:00,  2.20it/s]\n",
      "INFO:root:话题：#胖猫事件带给我们的反思才刚刚开始#，类型：综合，一共有 8页，已经解析完毕！\n",
      "INFO:root:Main body data saved to ./#胖猫事件带给我们的反思才刚刚开始#/demo.csv.\n",
      "INFO:root:Found 78 main body entries, starting level 1 comments parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing main body: 100%|██████████| 78/78 [00:47<00:00,  1.64it/s]\n",
      "INFO:root:Completed parsing all 78 main body entries for level 1 comments.\n",
      "INFO:root:Data successfully saved to ./#胖猫事件带给我们的反思才刚刚开始#/demo_comments_one.csv.\n",
      "INFO:root:Found 415 level 1 comments, starting level 2 parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing level 1 comments: 100%|██████████| 415/415 [02:53<00:00,  2.39it/s]\n",
      "INFO:root:Completed parsing all 415 level 1 comments for level 2 comments.\n",
      "INFO:root:Data successfully saved to ./#胖猫事件带给我们的反思才刚刚开始#/demo_comments_two.csv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minutes break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:00<00:00,  1.00s/it]\n",
      "INFO:root:Successfully created root path: #胖猫案细节公布#!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:话题：#胖猫案细节公布#，类型：综合，解析成功，一共有35页，准备开始解析...\n",
      "解析中...: 100%|██████████| 34/34 [00:23<00:00,  1.46it/s]\n",
      "INFO:root:话题：#胖猫案细节公布#，类型：综合，一共有35页，已经解析完毕！\n",
      "INFO:root:Main body data saved to ./#胖猫案细节公布#/demo.csv.\n",
      "INFO:root:Found 339 main body entries, starting level 1 comments parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing main body: 100%|██████████| 339/339 [19:49<00:00,  3.51s/it]   \n",
      "INFO:root:Completed parsing all 339 main body entries for level 1 comments.\n",
      "INFO:root:Data successfully saved to ./#胖猫案细节公布#/demo_comments_one.csv.\n",
      "INFO:root:Found 5311 level 1 comments, starting level 2 parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing level 1 comments:   0%|          | 1/5311 [16:01<1418:06:05, 961.42s/it]\n",
      "ERROR:root:Error during level 2 comments parsing: 解析页面失败，请检查你的cookie是否正确！\n",
      "INFO:root:Data successfully saved to ./#胖猫案细节公布#/demo_comments_two.csv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minutes break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:00<00:00,  1.00s/it]\n",
      "INFO:root:Successfully created root path: #胖猫事件#!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:话题：#胖猫事件#，类型：综合，解析成功，一共有47页，准备开始解析...\n",
      "解析中...: 100%|██████████| 46/46 [00:22<00:00,  2.01it/s]\n",
      "INFO:root:话题：#胖猫事件#，类型：综合，一共有47页，已经解析完毕！\n",
      "INFO:root:Main body data saved to ./#胖猫事件#/demo.csv.\n",
      "INFO:root:Found 370 main body entries, starting level 1 comments parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing main body:   1%|          | 4/370 [15:11<23:09:57, 227.86s/it]\n",
      "ERROR:root:Error during level 1 comments parsing: 解析页面失败，请检查你的cookie是否正确！\n",
      "INFO:root:Data successfully saved to ./#胖猫事件#/demo_comments_one.csv.\n",
      "INFO:root:Found 953 level 1 comments, starting level 2 parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing level 1 comments:  44%|████▎     | 415/953 [03:44<04:51,  1.84it/s]\n",
      "ERROR:root:Error during level 2 comments parsing: 解析页面失败，请检查你的cookie是否正确！\n",
      "INFO:root:Data successfully saved to ./#胖猫事件#/demo_comments_two.csv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minutes break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:00<00:00,  1.00s/it]\n",
      "INFO:root:Successfully created root path: #警方通报胖猫事件调查情况#!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:话题：#警方通报胖猫事件调查情况#，类型：综合，解析成功，一共有38页，准备开始解析...\n",
      "解析中...: 100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "INFO:root:话题：#警方通报胖猫事件调查情况#，类型：综合，一共有38页，已经解析完毕！\n",
      "INFO:root:Main body data saved to ./#警方通报胖猫事件调查情况#/demo.csv.\n",
      "INFO:root:Found 347 main body entries, starting level 1 comments parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing main body: 100%|██████████| 347/347 [14:56<00:00,  2.58s/it]  \n",
      "INFO:root:Completed parsing all 347 main body entries for level 1 comments.\n",
      "INFO:root:Data successfully saved to ./#警方通报胖猫事件调查情况#/demo_comments_one.csv.\n",
      "INFO:root:Found 3694 level 1 comments, starting level 2 parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing level 1 comments:  51%|█████▏    | 1896/3694 [1:37:37<1:32:35,  3.09s/it]   \n",
      "ERROR:root:Error during level 2 comments parsing: 解析页面失败，请检查你的cookie是否正确！\n",
      "INFO:root:Data successfully saved to ./#警方通报胖猫事件调查情况#/demo_comments_two.csv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minutes break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:00<00:00,  1.00s/it]\n",
      "INFO:root:Successfully created root path: #胖猫谭竹#!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:话题：#胖猫谭竹#，类型：综合，解析成功，一共有44页，准备开始解析...\n",
      "解析中...: 100%|██████████| 43/43 [00:19<00:00,  2.25it/s]\n",
      "INFO:root:话题：#胖猫谭竹#，类型：综合，一共有44页，已经解析完毕！\n",
      "INFO:root:Main body data saved to ./#胖猫谭竹#/demo.csv.\n",
      "INFO:root:Found 412 main body entries, starting level 1 comments parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing main body:   0%|          | 2/412 [02:09<7:23:10, 64.86s/it]\n",
      "ERROR:root:Error during level 1 comments parsing: 解析页面失败，请检查你的cookie是否正确！\n",
      "INFO:root:Data successfully saved to ./#胖猫谭竹#/demo_comments_one.csv.\n",
      "INFO:root:Found 142 level 1 comments, starting level 2 parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing level 1 comments:  16%|█▌        | 23/142 [04:34<23:42, 11.95s/it]\n",
      "ERROR:root:Error during level 2 comments parsing: 解析页面失败，请检查你的cookie是否正确！\n",
      "INFO:root:Data successfully saved to ./#胖猫谭竹#/demo_comments_two.csv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minutes break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:00<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "for q in themes:\n",
    "    wbparser = WBParser(cookie, q)\n",
    "    # 获取主题内容\n",
    "    print(\"Main body\")\n",
    "    wbparser.get_main_body(q, kind)\n",
    "    # 获取一级评论\n",
    "    print(\"Level 1 comments\")\n",
    "    wbparser.get_comments_level_one()\n",
    "    # 获取二级评论\n",
    "    print(\"Level 2 comments\")\n",
    "    wbparser.get_comments_level_two()\n",
    "    \n",
    "    print(\"1 minutes break\")\n",
    "    for i in tqdm(range(60)):\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计每个话题一级和二级评论\n",
    "因为分了好几次爬虫，每个一级和二级评论会分开保存，但是合并的保存不一定完整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#警方通报胖猫事件调查情况#\n",
      "Comment level 1:\n",
      "There are 3694 comments in total.\n",
      "Comment level 2:\n",
      "There are 5571 comments in total.\n",
      "#胖猫谭竹#\n",
      "Comment level 1:\n",
      "There are 7328 comments in total.\n",
      "Comment level 2:\n",
      "没有找到 CSV 文件在文件夹: /home1/zhangxiao/projects/crawl/saved2/#胖猫谭竹#/Comments_level_2\n",
      "No level 2 comments\n",
      "#胖猫姐姐刘某操控舆论#\n",
      "Comment level 1:\n",
      "There are 6663 comments in total.\n",
      "Comment level 2:\n",
      "There are 3180 comments in total.\n",
      "#胖猫事件#\n",
      "Comment level 1:\n",
      "There are 8255 comments in total.\n",
      "Comment level 2:\n",
      "There are 411 comments in total.\n",
      "#胖猫事件带给我们的反思才刚刚开始#\n",
      "Comment level 1:\n",
      "There are 415 comments in total.\n",
      "Comment level 2:\n",
      "There are 134 comments in total.\n",
      "#胖猫案细节公布#\n",
      "Comment level 1:\n",
      "There are 5311 comments in total.\n",
      "Comment level 2:\n",
      "There are 442 comments in total.\n",
      "level 1 comments: 31666\n",
      "level 2 comments: 9738\n"
     ]
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "# cur_dir = \"saved2\"\n",
    "# print(cur_dir)\n",
    "comments_1 = 0 \n",
    "comments_2 = 0\n",
    "for dir in os.listdir(cur_dir):\n",
    "    # print(dir)\n",
    "    if dir.startswith('#'):\n",
    "        print(dir)\n",
    "        print(\"Comment level 1:\")\n",
    "        comment1_path = os.path.join(cur_dir, dir, \"Comments_level_1\")\n",
    "        comment1_out = os.path.join(cur_dir, dir, \"1_demo_comments.csv\")\n",
    "        comments_1 += merge_csv_files(comment1_path, comment1_out)\n",
    "        try:\n",
    "            print(\"Comment level 2:\")\n",
    "            comment2_path = os.path.join(cur_dir, dir, \"Comments_level_2\")\n",
    "            comment2_out = os.path.join(cur_dir, dir, \"2_demo_comments.csv\")\n",
    "            comments_2 += merge_csv_files(comment2_path, comment2_out)\n",
    "        except:\n",
    "            print(\"No level 2 comments\")\n",
    "\n",
    "print(f\"level 1 comments: {comments_1}\")\n",
    "print(f\"level 2 comments: {comments_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_body_mid</th>\n",
       "      <th>main_body_uid</th>\n",
       "      <th>发布时间</th>\n",
       "      <th>处理内容</th>\n",
       "      <th>评论地点</th>\n",
       "      <th>mid</th>\n",
       "      <th>回复数量</th>\n",
       "      <th>点赞数量</th>\n",
       "      <th>原生内容</th>\n",
       "      <th>uid</th>\n",
       "      <th>...</th>\n",
       "      <th>用户地理位置</th>\n",
       "      <th>用户性别</th>\n",
       "      <th>用户粉丝数量</th>\n",
       "      <th>用户关注数量</th>\n",
       "      <th>用户全部微博</th>\n",
       "      <th>用户累计评论</th>\n",
       "      <th>用户累计转发</th>\n",
       "      <th>用户累计获赞</th>\n",
       "      <th>用户转评赞</th>\n",
       "      <th>用户认证信息</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5032806996771137</td>\n",
       "      <td>7513711425</td>\n",
       "      <td>24年05月11日 11:31</td>\n",
       "      <td>小三姑你跳河我会伤心的</td>\n",
       "      <td>来自贵州</td>\n",
       "      <td>5032807607304440</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>小三姑你跳河我会伤心的</td>\n",
       "      <td>7914578798</td>\n",
       "      <td>...</td>\n",
       "      <td>贵州</td>\n",
       "      <td>m</td>\n",
       "      <td>116</td>\n",
       "      <td>398</td>\n",
       "      <td>112</td>\n",
       "      <td>696</td>\n",
       "      <td>7</td>\n",
       "      <td>586</td>\n",
       "      <td>1,289</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5032806996771137</td>\n",
       "      <td>7513711425</td>\n",
       "      <td>24年05月11日 11:30</td>\n",
       "      <td>你没事吧&lt;img alt=\"[二哈]\" title=\"[二哈]\" src=\"https://...</td>\n",
       "      <td>来自上海</td>\n",
       "      <td>5032807418824048</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>你没事吧[二哈]</td>\n",
       "      <td>6530018710</td>\n",
       "      <td>...</td>\n",
       "      <td>海外</td>\n",
       "      <td>m</td>\n",
       "      <td>653</td>\n",
       "      <td>48</td>\n",
       "      <td>287</td>\n",
       "      <td>477</td>\n",
       "      <td>53</td>\n",
       "      <td>1,172</td>\n",
       "      <td>1,702</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      main_body_mid  main_body_uid             发布时间  \\\n",
       "0  5032806996771137     7513711425  24年05月11日 11:31   \n",
       "1  5032806996771137     7513711425  24年05月11日 11:30   \n",
       "\n",
       "                                                处理内容  评论地点               mid  \\\n",
       "0                                        小三姑你跳河我会伤心的  来自贵州  5032807607304440   \n",
       "1  你没事吧<img alt=\"[二哈]\" title=\"[二哈]\" src=\"https://...  来自上海  5032807418824048   \n",
       "\n",
       "   回复数量  点赞数量         原生内容         uid  ... 用户地理位置 用户性别 用户粉丝数量 用户关注数量 用户全部微博  \\\n",
       "0     2     0  小三姑你跳河我会伤心的  7914578798  ...     贵州    m    116    398    112   \n",
       "1     0     0     你没事吧[二哈]  6530018710  ...     海外    m    653     48    287   \n",
       "\n",
       "   用户累计评论  用户累计转发  用户累计获赞  用户转评赞 用户认证信息  \n",
       "0     696       7     586  1,289    NaN  \n",
       "1     477      53   1,172  1,702    NaN  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将以及评论合并到一起\n",
    "df = pd.read_excel(\"/home1/zhangxiao/projects/crawl/final/comment_level_1.xlsx\")\n",
    "# print(\"unnamed\" in df)\n",
    "print(df.columns[0])\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将获取的csv 结果按话题保存到excel中\n",
    "为了重新测试爬虫，备份了一次，将现有数据和备份数据进行合并然后进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home1/zhangxiao/projects/crawl\"\n",
    "\n",
    "def process_comments_for_theme(save_paths, theme, level):\n",
    "    \"\"\"\n",
    "    处理指定话题下的评论数据，并返回合并后的 DataFrame 和该话题的统计数据。\n",
    "    \n",
    "    参数:\n",
    "    - save_paths: 保存文件路径列表。\n",
    "    - theme: 当前话题。\n",
    "    - level: 评论的层级（0为demo，1和2为一级、二级评论）。\n",
    "    \n",
    "    返回:\n",
    "    - merged_df: 合并后的 DataFrame。\n",
    "    - total_items: 合并后的评论条数。\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    seperate_items = []\n",
    "\n",
    "    for s in save_paths:\n",
    "        comment_csv = os.path.join(s, theme, f\"{'demo' if level == 0 else f'{level}_demo_comments'}.csv\")\n",
    "        try:\n",
    "            df = pd.read_csv(comment_csv)\n",
    "            # print(f\"{s.split('/')[-1]}: {len(df)} items for theme: {theme}\")\n",
    "            data_list.append(df)\n",
    "            seperate_items.append(len(df))\n",
    "        except:\n",
    "            print(f\"No such csv file: {comment_csv}\")\n",
    "\n",
    "    \n",
    "\n",
    "    if data_list:\n",
    "        # 合并并去重\n",
    "        merged_df = pd.concat(data_list).reset_index(drop=True).astype(str)\n",
    "        if \"Unnamed: 0\" in merged_df.columns:\n",
    "            merged_df = merged_df.drop(columns=\"Unnamed: 0\")\n",
    "        merged_df = merged_df.drop_duplicates()\n",
    "        total_items = len(merged_df)\n",
    "        return merged_df, seperate_items, total_items\n",
    "    else:\n",
    "        print(f\"No data to merge for theme: {theme}\")\n",
    "        return pd.DataFrame(),None, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_comments(level,output_path=None):\n",
    "    \"\"\"\n",
    "    根据指定的评论层级，处理所有话题的评论数据，并保存统计信息到一个 DataFrame 中。\n",
    "    \n",
    "    参数:\n",
    "    - level: 评论层级，0为demo，1和2为一级、二级评论。\n",
    "    \n",
    "    返回:\n",
    "    - stats_df: 统计信息的 DataFrame。\n",
    "    \"\"\"\n",
    "    save_paths = [os.path.join(root_path, \"saved\"), os.path.join(root_path, \"saved2\")]\n",
    "    themes = [\n",
    "        '#胖猫姐姐刘某操控舆论#', \n",
    "        '#胖猫事件带给我们的反思才刚刚开始#', \n",
    "        '#胖猫案细节公布#', \n",
    "        '#胖猫事件#', \n",
    "        '#警方通报胖猫事件调查情况#', \n",
    "        '#胖猫谭竹#'\n",
    "    ]\n",
    "\n",
    "    # 初始化用于保存统计数据的 DataFrame\n",
    "    # stats_df = pd.DataFrame(columns=['Theme', 'Total_Comments'])\n",
    "    total_number = 0\n",
    "    print(f\"Processing {'demo' if level == 0 else f'level {level}'} comments\")\n",
    "    \n",
    "    # 仅供观赏，dataframe 保存统计结果\n",
    "    stats_dict = dict()\n",
    "    \n",
    "    for theme in themes:\n",
    "        # print(f\"Processing theme: {theme}\")\n",
    "        merged_df, sep_items, total_items = process_comments_for_theme(save_paths, theme, level)\n",
    "        stats_dict[theme] = {\n",
    "            'total_num': total_items\n",
    "        }\n",
    "        for i in range(len(sep_items)):\n",
    "            stats_dict[theme][f'num items{i+1}'] = sep_items[i]\n",
    "        # print(theme, sep_items, total_items)\n",
    "        total_number += total_items\n",
    "        if output_path is not None:\n",
    "            merged_df.to_excel(output_path)\n",
    "        # 将统计结果保存到 DataFrame 中\n",
    "        # stats_df = stats_df({'Theme': theme, 'Total_Comments': total_items}, ignore_index=True)\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_dict).T\n",
    "    stats_df = stats_df.fillna(0).astype(int)\n",
    "    # print(stats_df)\n",
    "    return total_number, stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing demo comments\n",
      "There are totally 2205 demos.\n",
      "Processing level 1 comments\n",
      "There are totally 39075 level 1 comments.\n",
      "Processing level 2 comments\n",
      "No such csv file: /home1/zhangxiao/projects/crawl/saved2/#胖猫谭竹#/2_demo_comments.csv\n",
      "There are totally 19086 level 2 comments.\n"
     ]
    }
   ],
   "source": [
    "# 示例调用，处理 demo 评论，保存统计信息到 DataFrame 中\n",
    "# 如果需要保存，在get_all_comments中添加output_path 就好了\n",
    "num_demo, demo_df = get_all_comments(0)\n",
    "print(f\"There are totally {num_demo} demos.\")\n",
    "num_com1, com1_df = get_all_comments(1)\n",
    "print(f\"There are totally {num_com1} level 1 comments.\")\n",
    "num_com2, com2_df = get_all_comments(2)\n",
    "print(f\"There are totally {num_com2} level 2 comments.\")\n",
    "# print(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_num</th>\n",
       "      <th>num items1</th>\n",
       "      <th>num items2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#胖猫姐姐刘某操控舆论#</th>\n",
       "      <td>421</td>\n",
       "      <td>414</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫事件带给我们的反思才刚刚开始#</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫案细节公布#</th>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫事件#</th>\n",
       "      <td>386</td>\n",
       "      <td>369</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#警方通报胖猫事件调查情况#</th>\n",
       "      <td>469</td>\n",
       "      <td>244</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫谭竹#</th>\n",
       "      <td>511</td>\n",
       "      <td>230</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    total_num  num items1  num items2\n",
       "#胖猫姐姐刘某操控舆论#              421         414         414\n",
       "#胖猫事件带给我们的反思才刚刚开始#         79          79          78\n",
       "#胖猫案细节公布#                 339         339         339\n",
       "#胖猫事件#                    386         369         365\n",
       "#警方通报胖猫事件调查情况#            469         244         347\n",
       "#胖猫谭竹#                    511         230         414"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_num</th>\n",
       "      <th>num items1</th>\n",
       "      <th>num items2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#胖猫姐姐刘某操控舆论#</th>\n",
       "      <td>9033</td>\n",
       "      <td>7296</td>\n",
       "      <td>6663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫事件带给我们的反思才刚刚开始#</th>\n",
       "      <td>491</td>\n",
       "      <td>416</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫案细节公布#</th>\n",
       "      <td>6277</td>\n",
       "      <td>5293</td>\n",
       "      <td>5311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫事件#</th>\n",
       "      <td>8448</td>\n",
       "      <td>494</td>\n",
       "      <td>8255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#警方通报胖猫事件调查情况#</th>\n",
       "      <td>5138</td>\n",
       "      <td>1535</td>\n",
       "      <td>3694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫谭竹#</th>\n",
       "      <td>9688</td>\n",
       "      <td>2629</td>\n",
       "      <td>7328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    total_num  num items1  num items2\n",
       "#胖猫姐姐刘某操控舆论#             9033        7296        6663\n",
       "#胖猫事件带给我们的反思才刚刚开始#        491         416         415\n",
       "#胖猫案细节公布#                6277        5293        5311\n",
       "#胖猫事件#                   8448         494        8255\n",
       "#警方通报胖猫事件调查情况#           5138        1535        3694\n",
       "#胖猫谭竹#                   9688        2629        7328"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_num</th>\n",
       "      <th>num items1</th>\n",
       "      <th>num items2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#胖猫姐姐刘某操控舆论#</th>\n",
       "      <td>5075</td>\n",
       "      <td>1895</td>\n",
       "      <td>3180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫事件带给我们的反思才刚刚开始#</th>\n",
       "      <td>145</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫案细节公布#</th>\n",
       "      <td>1985</td>\n",
       "      <td>1912</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫事件#</th>\n",
       "      <td>499</td>\n",
       "      <td>348</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#警方通报胖猫事件调查情况#</th>\n",
       "      <td>7985</td>\n",
       "      <td>2563</td>\n",
       "      <td>5571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#胖猫谭竹#</th>\n",
       "      <td>3397</td>\n",
       "      <td>3397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    total_num  num items1  num items2\n",
       "#胖猫姐姐刘某操控舆论#             5075        1895        3180\n",
       "#胖猫事件带给我们的反思才刚刚开始#        145         134         134\n",
       "#胖猫案细节公布#                1985        1912         442\n",
       "#胖猫事件#                    499         348         411\n",
       "#警方通报胖猫事件调查情况#           7985        2563        5571\n",
       "#胖猫谭竹#                   3397        3397           0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将每个话题的数据统计到同一excel文件下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 38102 level 1 comments.\n",
      "There are totally 19086 level 2 comments.\n",
      "There are totally 2807 demos.\n"
     ]
    }
   ],
   "source": [
    "final_path = os.getcwd() + \"/final\"\n",
    "comment_1_list = []\n",
    "comment_2_list = []\n",
    "demo_list = []\n",
    "\n",
    "for file in os.listdir(final_path):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(final_path, file))\n",
    "        if file.endswith(\"1.xlsx\"):\n",
    "            comment_1_list.append(df)\n",
    "        elif file.endswith(\"2.xlsx\"):\n",
    "            comment_2_list.append(df)\n",
    "        elif file.endswith(\"demo.xlsx\"):\n",
    "            demo_list.append(df)\n",
    "\n",
    "merged_df1 = pd.concat(comment_1_list).reset_index(drop=True).astype(str).drop(columns=\"Unnamed: 0\").drop_duplicates()\n",
    "merged_df2 = pd.concat(comment_2_list).reset_index(drop=True).astype(str).drop(columns=\"Unnamed: 0\").drop_duplicates()\n",
    "merged_demo = pd.concat(demo_list).reset_index(drop=True).astype(str).drop(columns=\"Unnamed: 0\").drop_duplicates()\n",
    "\n",
    "print(f\"There are totally {len(merged_df1)} level 1 comments.\")\n",
    "print(f\"There are totally {len(merged_df2)} level 2 comments.\")\n",
    "print(f\"There are totally {len(merged_demo)} demos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_body_mid</th>\n",
       "      <th>main_body_uid</th>\n",
       "      <th>发布时间</th>\n",
       "      <th>处理内容</th>\n",
       "      <th>评论地点</th>\n",
       "      <th>mid</th>\n",
       "      <th>回复数量</th>\n",
       "      <th>点赞数量</th>\n",
       "      <th>原生内容</th>\n",
       "      <th>uid</th>\n",
       "      <th>...</th>\n",
       "      <th>用户地理位置</th>\n",
       "      <th>用户性别</th>\n",
       "      <th>用户粉丝数量</th>\n",
       "      <th>用户关注数量</th>\n",
       "      <th>用户全部微博</th>\n",
       "      <th>用户累计评论</th>\n",
       "      <th>用户累计转发</th>\n",
       "      <th>用户累计获赞</th>\n",
       "      <th>用户转评赞</th>\n",
       "      <th>用户认证信息</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5032806996771137</td>\n",
       "      <td>7513711425</td>\n",
       "      <td>24年05月11日 11:31</td>\n",
       "      <td>小三姑你跳河我会伤心的</td>\n",
       "      <td>来自贵州</td>\n",
       "      <td>5032807607304440</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>小三姑你跳河我会伤心的</td>\n",
       "      <td>7914578798</td>\n",
       "      <td>...</td>\n",
       "      <td>贵州</td>\n",
       "      <td>m</td>\n",
       "      <td>116</td>\n",
       "      <td>398</td>\n",
       "      <td>112</td>\n",
       "      <td>696</td>\n",
       "      <td>7</td>\n",
       "      <td>586</td>\n",
       "      <td>1,289</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5032806996771137</td>\n",
       "      <td>7513711425</td>\n",
       "      <td>24年05月11日 11:30</td>\n",
       "      <td>你没事吧&lt;img alt=\"[二哈]\" title=\"[二哈]\" src=\"https://...</td>\n",
       "      <td>来自上海</td>\n",
       "      <td>5032807418824048</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>你没事吧[二哈]</td>\n",
       "      <td>6530018710</td>\n",
       "      <td>...</td>\n",
       "      <td>海外</td>\n",
       "      <td>m</td>\n",
       "      <td>653</td>\n",
       "      <td>48</td>\n",
       "      <td>287</td>\n",
       "      <td>477</td>\n",
       "      <td>53</td>\n",
       "      <td>1,172</td>\n",
       "      <td>1,702</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5031558065161616</td>\n",
       "      <td>6013715279</td>\n",
       "      <td>24年05月08日 00:56</td>\n",
       "      <td>干脆直接单身吧这些男的</td>\n",
       "      <td>来自江苏</td>\n",
       "      <td>5031560537703260</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>干脆直接单身吧这些男的</td>\n",
       "      <td>7442435940</td>\n",
       "      <td>...</td>\n",
       "      <td>其他</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5031367307690922</td>\n",
       "      <td>2400903283</td>\n",
       "      <td>24年05月08日 06:36</td>\n",
       "      <td>转行情感博主了</td>\n",
       "      <td>来自加拿大</td>\n",
       "      <td>5031646275043522</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>转行情感博主了</td>\n",
       "      <td>2487758140</td>\n",
       "      <td>...</td>\n",
       "      <td>海外 加拿大</td>\n",
       "      <td>m</td>\n",
       "      <td>262</td>\n",
       "      <td>746</td>\n",
       "      <td>1289</td>\n",
       "      <td>630</td>\n",
       "      <td>198</td>\n",
       "      <td>182</td>\n",
       "      <td>1,010</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5031367307690922</td>\n",
       "      <td>2400903283</td>\n",
       "      <td>24年05月07日 12:56</td>\n",
       "      <td>真的，我最想说的就是，爱人先爱己，你的人生不是只有另一半，生命很重要也很脆弱，希望大家都可以...</td>\n",
       "      <td>来自河南</td>\n",
       "      <td>5031379446269610</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>真的，我最想说的就是，爱人先爱己，你的人生不是只有另一半，生命很重要也很脆弱，希望大家都可以...</td>\n",
       "      <td>6977404515</td>\n",
       "      <td>...</td>\n",
       "      <td>其他</td>\n",
       "      <td>m</td>\n",
       "      <td>13</td>\n",
       "      <td>831</td>\n",
       "      <td>801</td>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "      <td>746</td>\n",
       "      <td>840</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5031367307690922</td>\n",
       "      <td>2400903283</td>\n",
       "      <td>24年05月07日 12:47</td>\n",
       "      <td>你真的说的很好&lt;img alt=\"[打call]\" title=\"[打call]\" src=...</td>\n",
       "      <td>来自湖北</td>\n",
       "      <td>5031377187901862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>你真的说的很好[打call]</td>\n",
       "      <td>6424201018</td>\n",
       "      <td>...</td>\n",
       "      <td>其他</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5031367307690922</td>\n",
       "      <td>2400903283</td>\n",
       "      <td>24年05月07日 12:46</td>\n",
       "      <td>说得好</td>\n",
       "      <td>来自河南</td>\n",
       "      <td>5031376889839837</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>说得好</td>\n",
       "      <td>7597954267</td>\n",
       "      <td>...</td>\n",
       "      <td>其他</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5032806801212580</td>\n",
       "      <td>7512842757</td>\n",
       "      <td>24年05月11日 11:33</td>\n",
       "      <td>支持谭竹</td>\n",
       "      <td>来自江苏</td>\n",
       "      <td>5032808093320739</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>支持谭竹</td>\n",
       "      <td>7292530205</td>\n",
       "      <td>...</td>\n",
       "      <td>北京</td>\n",
       "      <td>m</td>\n",
       "      <td>41</td>\n",
       "      <td>77</td>\n",
       "      <td>2644</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>113</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5032806801212580</td>\n",
       "      <td>7512842757</td>\n",
       "      <td>24年05月11日 20:05</td>\n",
       "      <td>有点幽默了，左上角的照片为什么不发原图，还要给他美颜呢&lt;img alt=\"[允悲]\" tit...</td>\n",
       "      <td>来自河南</td>\n",
       "      <td>5032936841674883</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>有点幽默了，左上角的照片为什么不发原图，还要给他美颜呢[允悲][允悲]，不会是觉得太稠拿不出...</td>\n",
       "      <td>7906565460</td>\n",
       "      <td>...</td>\n",
       "      <td>其他</td>\n",
       "      <td>f</td>\n",
       "      <td>8</td>\n",
       "      <td>93</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5032806801212580</td>\n",
       "      <td>7512842757</td>\n",
       "      <td>24年05月11日 11:31</td>\n",
       "      <td>怎么天天都是他</td>\n",
       "      <td>来自江苏</td>\n",
       "      <td>5032807472827574</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>怎么天天都是他</td>\n",
       "      <td>3107567797</td>\n",
       "      <td>...</td>\n",
       "      <td>广东 汕头</td>\n",
       "      <td>m</td>\n",
       "      <td>8</td>\n",
       "      <td>93</td>\n",
       "      <td>4203</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      main_body_mid main_body_uid             发布时间  \\\n",
       "0  5032806996771137    7513711425  24年05月11日 11:31   \n",
       "1  5032806996771137    7513711425  24年05月11日 11:30   \n",
       "2  5031558065161616    6013715279  24年05月08日 00:56   \n",
       "3  5031367307690922    2400903283  24年05月08日 06:36   \n",
       "4  5031367307690922    2400903283  24年05月07日 12:56   \n",
       "5  5031367307690922    2400903283  24年05月07日 12:47   \n",
       "6  5031367307690922    2400903283  24年05月07日 12:46   \n",
       "7  5032806801212580    7512842757  24年05月11日 11:33   \n",
       "8  5032806801212580    7512842757  24年05月11日 20:05   \n",
       "9  5032806801212580    7512842757  24年05月11日 11:31   \n",
       "\n",
       "                                                处理内容   评论地点               mid  \\\n",
       "0                                        小三姑你跳河我会伤心的   来自贵州  5032807607304440   \n",
       "1  你没事吧<img alt=\"[二哈]\" title=\"[二哈]\" src=\"https://...   来自上海  5032807418824048   \n",
       "2                                        干脆直接单身吧这些男的   来自江苏  5031560537703260   \n",
       "3                                            转行情感博主了  来自加拿大  5031646275043522   \n",
       "4  真的，我最想说的就是，爱人先爱己，你的人生不是只有另一半，生命很重要也很脆弱，希望大家都可以...   来自河南  5031379446269610   \n",
       "5  你真的说的很好<img alt=\"[打call]\" title=\"[打call]\" src=...   来自湖北  5031377187901862   \n",
       "6                                                说得好   来自河南  5031376889839837   \n",
       "7                                               支持谭竹   来自江苏  5032808093320739   \n",
       "8  有点幽默了，左上角的照片为什么不发原图，还要给他美颜呢<img alt=\"[允悲]\" tit...   来自河南  5032936841674883   \n",
       "9                                            怎么天天都是他   来自江苏  5032807472827574   \n",
       "\n",
       "  回复数量 点赞数量                                               原生内容         uid  \\\n",
       "0    2    0                                        小三姑你跳河我会伤心的  7914578798   \n",
       "1    0    0                                           你没事吧[二哈]  6530018710   \n",
       "2    0    1                                        干脆直接单身吧这些男的  7442435940   \n",
       "3    3    1                                            转行情感博主了  2487758140   \n",
       "4    0    2  真的，我最想说的就是，爱人先爱己，你的人生不是只有另一半，生命很重要也很脆弱，希望大家都可以...  6977404515   \n",
       "5    0    0                                     你真的说的很好[打call]  6424201018   \n",
       "6    0    0                                                说得好  7597954267   \n",
       "7    6   52                                               支持谭竹  7292530205   \n",
       "8    6    6  有点幽默了，左上角的照片为什么不发原图，还要给他美颜呢[允悲][允悲]，不会是觉得太稠拿不出...  7906565460   \n",
       "9    2   10                                            怎么天天都是他  3107567797   \n",
       "\n",
       "   ...  用户地理位置 用户性别 用户粉丝数量 用户关注数量 用户全部微博 用户累计评论 用户累计转发 用户累计获赞  用户转评赞 用户认证信息  \n",
       "0  ...      贵州    m    116    398    112    696      7    586  1,289    nan  \n",
       "1  ...      海外    m    653     48    287    477     53  1,172  1,702    nan  \n",
       "2  ...      其他    m      0     74      7      0      0     17     17    nan  \n",
       "3  ...  海外 加拿大    m    262    746   1289    630    198    182  1,010    nan  \n",
       "4  ...      其他    m     13    831    801     89      5    746    840    nan  \n",
       "5  ...      其他    m      1      8      9      4      0      3      7    nan  \n",
       "6  ...      其他    f      1      2      0      0      0      0      0    nan  \n",
       "7  ...      北京    m     41     77   2644     18      3     92    113    nan  \n",
       "8  ...      其他    f      8     93     33     13     26     12     51    nan  \n",
       "9  ...   广东 汕头    m      8     93   4203     14      1     40     55    nan  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved level 1 comments in /home1/zhangxiao/projects/crawl/final/all_together/comment_level_1.xlsx\n",
      "Successfully saved level 2 comments in /home1/zhangxiao/projects/crawl/final/all_together/comment_level_2.xlsx\n",
      "Successfully saved demo in /home1/zhangxiao/projects/crawl/final/all_together/demo.xlsx\n"
     ]
    }
   ],
   "source": [
    "c1_excel = os.path.join(final_path, \"all_together\", \"comment_level_1.xlsx\")\n",
    "merged_df1.to_excel(c1_excel)\n",
    "print(f\"Successfully saved level 1 comments in {c1_excel}\")\n",
    "\n",
    "c2_excel = os.path.join(final_path, \"all_together\", \"comment_level_2.xlsx\")\n",
    "merged_df1.to_excel(c2_excel)\n",
    "print(f\"Successfully saved level 2 comments in {c2_excel}\")\n",
    "\n",
    "demo_excel = os.path.join(final_path, \"all_together\", \"demo.xlsx\")\n",
    "merged_df1.to_excel(demo_excel)\n",
    "print(f\"Successfully saved demo in {demo_excel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98ccfcc13dbcadb4032b4d53b4839f8d4d884911ec41bc447164cac5155f0cd2"
  },
  "kernelspec": {
   "display_name": "Python 3.12.4 ('crawl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
